{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import timm\n",
    "from tqdm import tqdm\n",
    "\n",
    "from pytorch_adapt.containers import Models, Optimizers\n",
    "from pytorch_adapt.datasets import DataloaderCreator, get_mnist_mnistm, SourceDataset, TargetDataset, CombinedSourceAndTargetDataset\n",
    "from pytorch_adapt.hooks import DANNHook\n",
    "from pytorch_adapt.models import Discriminator, mnistC, mnistG, Classifier\n",
    "from pytorch_adapt.utils.common_functions import batch_to_device\n",
    "from pytorch_adapt.validators import IMValidator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Dataset import OfficeHome\n",
    "from util import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "domains = [\"Art\", \"Clipart\", \"Product\", \"Real World\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_train_dataloader = utils.get_train_loader(domains[0])\n",
    "src_test_dataloader = utils.get_test_loader(domains[0])\n",
    "tgt_train_dataloader = utils.get_train_loader(domains[1])\n",
    "tgt_test_dataloader = utils.get_test_loader(domains[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No pretrained weights exist for this model. Using random initialization.\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\")\n",
    "\n",
    "# G = mnistG(pretrained=True).to(device)\n",
    "G = timm.create_model('mobilenetv3_small_075', pretrained=True).to(device)\n",
    "G.classifier = nn.Identity()\n",
    "C = Classifier(in_size=1024, num_classes=10).to(device)\n",
    "D = Discriminator(in_size=1024, h=256).to(device)\n",
    "models = Models({\"G\": G, \"C\": C, \"D\": D})\n",
    "optimizers = Optimizers((torch.optim.Adam, {\"lr\": 0.0001}))\n",
    "optimizers.create_with(models)\n",
    "optimizers = list(optimizers.values())\n",
    "\n",
    "hook = DANNHook(optimizers)\n",
    "validator = IMValidator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_train_dataset = SourceDataset(src_train_dataloader.dataset)\n",
    "src_test_dataset = SourceDataset(src_test_dataloader.dataset)\n",
    "tgt_train_dataset = TargetDataset(tgt_train_dataloader.dataset)\n",
    "tgt_test_dataset = TargetDataset(tgt_test_dataloader.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_datasets = {\n",
    "    \"src_train\": src_train_dataset,\n",
    "    \"src_val\": src_test_dataset,\n",
    "    \"target_train\": tgt_train_dataset,\n",
    "    \"target_val\": tgt_test_dataset,\n",
    "    \"train\": CombinedSourceAndTargetDataset(source_dataset=src_train_dataset, target_dataset=tgt_train_dataset),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "dc = DataloaderCreator(batch_size=32, num_workers=2)\n",
    "dataloaders = dc(**custom_datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12/12 [00:10<00:00,  1.18it/s]\n",
      "100%|██████████| 13/13 [00:05<00:00,  2.32it/s]\n",
      "  0%|          | 0/12 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 score = 4.76837158203125e-07\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12/12 [00:09<00:00,  1.32it/s]\n",
      "100%|██████████| 13/13 [00:05<00:00,  2.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 score = 0.026787281036376953\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(2):\n",
    "\n",
    "    # train loop\n",
    "    models.train()\n",
    "    for data in tqdm(dataloaders[\"train\"]):\n",
    "        data = batch_to_device(data, device)\n",
    "        loss, _ = hook({}, {**models, **data})\n",
    "\n",
    "    # eval loop\n",
    "    models.eval()\n",
    "    logits = []\n",
    "    with torch.no_grad():\n",
    "        for data in tqdm(dataloaders[\"target_train\"]):\n",
    "            data = batch_to_device(data, device)\n",
    "            logits.append(C(G(data[\"target_imgs\"])))\n",
    "        logits = torch.cat(logits, dim=0)\n",
    "\n",
    "    # validation score\n",
    "    score = validator.score(target_train={\"logits\": logits})\n",
    "    print(f\"Epoch {epoch} score = {score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CombinedSourceAndTargetDataset(\n",
       "  (source_dataset): SourceDataset(\n",
       "    domain=0\n",
       "    (dataset): ConcatDataset(\n",
       "      len=60000\n",
       "      (datasets): [Dataset MNIST\n",
       "          Number of datapoints: 60000\n",
       "          Root location: .\n",
       "          Split: Train\n",
       "          StandardTransform\n",
       "      Transform: Compose(\n",
       "                     Resize(size=32, interpolation=bilinear)\n",
       "                     ToTensor()\n",
       "                     <pytorch_adapt.utils.transforms.GrayscaleToRGB object at 0x0000021387F6FF08>\n",
       "                     Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
       "                 )]\n",
       "    )\n",
       "  )\n",
       "  (target_dataset): TargetDataset(\n",
       "    domain=1\n",
       "    (dataset): ConcatDataset(\n",
       "      len=59001\n",
       "      (datasets): [MNISTM(\n",
       "        domain=MNISTM\n",
       "        len=59001\n",
       "        (transform): Compose(\n",
       "            ToTensor()\n",
       "            Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
       "        )\n",
       "      )]\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataloaders[\"train\"].dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'src_train': SourceDataset(\n",
       "   domain=0\n",
       "   (dataset): ConcatDataset(\n",
       "     len=60000\n",
       "     (datasets): [Dataset MNIST\n",
       "         Number of datapoints: 60000\n",
       "         Root location: .\n",
       "         Split: Train\n",
       "         StandardTransform\n",
       "     Transform: Compose(\n",
       "                    Resize(size=32, interpolation=bilinear)\n",
       "                    ToTensor()\n",
       "                    <pytorch_adapt.utils.transforms.GrayscaleToRGB object at 0x00000213882897C8>\n",
       "                    Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
       "                )]\n",
       "   )\n",
       " ),\n",
       " 'src_val': SourceDataset(\n",
       "   domain=0\n",
       "   (dataset): ConcatDataset(\n",
       "     len=10000\n",
       "     (datasets): [Dataset MNIST\n",
       "         Number of datapoints: 10000\n",
       "         Root location: .\n",
       "         Split: Test\n",
       "         StandardTransform\n",
       "     Transform: Compose(\n",
       "                    Resize(size=32, interpolation=bilinear)\n",
       "                    ToTensor()\n",
       "                    <pytorch_adapt.utils.transforms.GrayscaleToRGB object at 0x00000213884177C8>\n",
       "                    Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
       "                )]\n",
       "   )\n",
       " ),\n",
       " 'target_train': TargetDataset(\n",
       "   domain=1\n",
       "   (dataset): ConcatDataset(\n",
       "     len=59001\n",
       "     (datasets): [MNISTM(\n",
       "       domain=MNISTM\n",
       "       len=59001\n",
       "       (transform): Compose(\n",
       "           ToTensor()\n",
       "           Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
       "       )\n",
       "     )]\n",
       "   )\n",
       " ),\n",
       " 'target_val': TargetDataset(\n",
       "   domain=1\n",
       "   (dataset): ConcatDataset(\n",
       "     len=9001\n",
       "     (datasets): [MNISTM(\n",
       "       domain=MNISTM\n",
       "       len=9001\n",
       "       (transform): Compose(\n",
       "           ToTensor()\n",
       "           Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
       "       )\n",
       "     )]\n",
       "   )\n",
       " ),\n",
       " 'train': CombinedSourceAndTargetDataset(\n",
       "   (source_dataset): SourceDataset(\n",
       "     domain=0\n",
       "     (dataset): ConcatDataset(\n",
       "       len=60000\n",
       "       (datasets): [Dataset MNIST\n",
       "           Number of datapoints: 60000\n",
       "           Root location: .\n",
       "           Split: Train\n",
       "           StandardTransform\n",
       "       Transform: Compose(\n",
       "                      Resize(size=32, interpolation=bilinear)\n",
       "                      ToTensor()\n",
       "                      <pytorch_adapt.utils.transforms.GrayscaleToRGB object at 0x0000021387F6FF08>\n",
       "                      Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
       "                  )]\n",
       "     )\n",
       "   )\n",
       "   (target_dataset): TargetDataset(\n",
       "     domain=1\n",
       "     (dataset): ConcatDataset(\n",
       "       len=59001\n",
       "       (datasets): [MNISTM(\n",
       "         domain=MNISTM\n",
       "         len=59001\n",
       "         (transform): Compose(\n",
       "             ToTensor()\n",
       "             Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
       "         )\n",
       "       )]\n",
       "     )\n",
       "   )\n",
       " )}"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "839efb5f58ddc648d447649035bde14787ba9332bae58e3f09da86e2a85c0f21"
  },
  "kernelspec": {
   "display_name": "Python 3.6.2 64-bit ('dann': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
